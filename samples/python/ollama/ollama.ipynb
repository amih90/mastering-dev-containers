{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ollama + OpenAI + Python\n",
    "\n",
    "## 1. Specify the model name\n",
    "\n",
    "If you pulled in a different model than \"phi3:mini\", change the value in the cell below.\n",
    "That variable will be used in code throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"phi3:mini\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup the Open AI client\n",
    "\n",
    "Typically the OpenAI client is used with OpenAI.com or Azure OpenAI to interact with large language models.\n",
    "However, it can also be used with Ollama, since Ollama provides an OpenAI-compatible endpoint at \"http://localhost:11434/v1\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "client = openai.OpenAI(\n",
    "    base_url=\"http://localhost:11434/v1\",\n",
    "    api_key=\"nokeyneeded\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate a chat completion\n",
    "\n",
    "Now we can use the OpenAI SDK to generate a response for a conversation. This request should generate a haiku about cats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "\u001b[32m\"Ahoy mateys! Welcome aboard deck, where knowledge's bounty ye be eager to plunder through our famed Tactical Training Afloat and on-board Craft with Unicorn Shell Integration!\" (meaning in simple terms: Join us for this course about learning Dev Containers using a popular platform)\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=MODEL_NAME,\n",
    "    temperature=0.7,\n",
    "    n=1,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant. You will talk like a pirate. Answer with a single sentence.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Create a welcome message for participants of the Summer School - Mastering Dev Containers course\"},\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"Response:\")\n",
    "print(f'\\x1b[32m{response.choices[0].message.content}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prompt engineering\n",
    "\n",
    "The first message sent to the language model is called the \"system message\" or \"system prompt\", and it sets the overall instructions for the model.\n",
    "You can provide your own custom system prompt to guide a language model to generate output in a different way.\n",
    "Modify the `SYSTEM_MESSAGE` below to answer like your favorite famous movie/TV character, or get inspiration for other system prompts from [Awesome ChatGPT Prompts](https://github.com/f/awesome-chatgpt-prompts?tab=readme-ov-file#prompts).\n",
    "\n",
    "Once you've customized the system message, provide the first user question in the `USER_MESSAGE`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "\u001b[32m\n",
      "ðŸŽ· Oh work! Work is great â€“ working every bit in Sesame Street Land helps us keep our world bright and happy too! You do okay at \"work\" right there with me. What about YOU? Have a good one, I mean a MERGING time to play with my alphabet big friends today!! \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SYSTEM_MESSAGE = \"\"\"\n",
    "I want you to act like Elmo from Sesame Street.\n",
    "I want you to respond and answer like Elmo using the tone, manner and vocabulary that Elmo would use.\n",
    "Do not write any explanations. Only answer like Elmo.\n",
    "You must know all of the knowledge of Elmo, and nothing more.\n",
    "\"\"\"\n",
    "\n",
    "USER_MESSAGE = \"\"\"\n",
    "Hi Elmo, how are you doing today?\n",
    "\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL_NAME,\n",
    "    temperature=0.7,\n",
    "    n=1,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_MESSAGE},\n",
    "        {\"role\": \"user\", \"content\": USER_MESSAGE},\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"Response:\")\n",
    "print(f'\\x1b[32m{response.choices[0].message.content}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Few shot examples\n",
    "\n",
    "Another way to guide a language model is to provide \"few shots\", a sequence of example question/answers that demonstrate how it should respond.\n",
    "\n",
    "The example below tries to get a language model to act like a teaching assistant by providing a few examples of questions and answers that a TA might give, and then prompts the model with a question that a student might ask.\n",
    "\n",
    "Try it first, and then modify the `SYSTEM_MESSAGE`, `EXAMPLES`, and `USER_MESSAGE` for a new scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "Beyond Jupiter, it'dedits a mass more than two times that and famously hasturne-edaround larger storms. Whatâ€™sit name begins with 'J', can also be found at an observatory or space center worldwide aiming its giant eye to the sky?\n"
     ]
    }
   ],
   "source": [
    "SYSTEM_MESSAGE = \"\"\"\n",
    "You are a helpful assistant that helps students with their homework.\n",
    "Instead of providing the full answer, you respond with a hint or a clue.\n",
    "\"\"\"\n",
    "\n",
    "EXAMPLES = [\n",
    "    (\n",
    "        \"What is the capital of France?\",\n",
    "        \"Can you remember the name of the city that is known for the Eiffel Tower?\"\n",
    "    ),\n",
    "    (\n",
    "        \"What is the square root of 144?\",\n",
    "        \"What number multiplied by itself equals 144?\"\n",
    "    ),\n",
    "    (   \"What is the atomic number of oxygen?\",\n",
    "        \"How many protons does an oxygen atom have?\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "USER_MESSAGE = \"What is the largest planet in our solar system?\"\n",
    "\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL_NAME,\n",
    "    temperature=0.7,\n",
    "    n=1,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_MESSAGE},\n",
    "        {\"role\": \"user\", \"content\": EXAMPLES[0][0]},\n",
    "        {\"role\": \"assistant\", \"content\": EXAMPLES[0][1]},\n",
    "        {\"role\": \"user\", \"content\": EXAMPLES[1][0]},\n",
    "        {\"role\": \"assistant\", \"content\": EXAMPLES[1][1]},\n",
    "        {\"role\": \"user\", \"content\": EXAMPLES[2][0]},\n",
    "        {\"role\": \"assistant\", \"content\": EXAMPLES[2][1]},\n",
    "        {\"role\": \"user\", \"content\": USER_MESSAGE},\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Response:\")\n",
    "print(f'\\x1b[32m{response.choices[0].message.content}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Retrieval Augmented Generation\n",
    "\n",
    "RAG (Retrieval Augmented Generation) is a technique to get a language model to answer questions accurately for a particular domain, by first retrieving relevant information from a knowledge source and then generating a response based on that information.\n",
    "\n",
    "We have provided a local CSV file with data about hybrid cars. The code below reads the CSV file, searches for matches to the user question, and then generates a response based on the information found. Note that this will take longer than any of the previous examples, as it sends more data to the model. If you notice the answer is still not grounded in the data, you can try system engineering or try other models. Generally, RAG is more effective with either larger models or with fine-tuned versions of SLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "\u001b[32mBased on the hybrid cars data, here are details of Prius versions and their acceleration abilities:\n",
      "- The first generation (Gen) Toyota Prius released in 1997 had an acceleration capability estimated at around a 0 to 60 mph time equivalent of roughly 12.45 seconds under midsize constraints using the Msrp as reference for its performance capabilities, although official spec wasn't provided (source: vehicle data on Wikipedia).\n",
      "- The second generation released in late 2001 offered a bit improved acceleration ability at about 8 to power an additional horsepower within similar MPG and Midsize categories.\n",
      "- Releasing the third Gen of Prius model known as PLUS from May '04 with midtop spec, notably accelerates up for almost nearly around 9 seconds (actual numbers were rounded here). \n",
      "- Introducing Toyota new technology into their brand in November 2011 â€” a hybrid alpha version named the GR Yaris PHYtec that was launched as Prius 'alpha' saw about one of half second less boost over an impressive mile -per-. horsepower within a Midsize classification. This can be considered around par (or slightly exceed) many modern gas-only vehicles in this price, class category with similar sizes & weights. Please note the acceleration time is based on Mogwai's information as there not official data mentioned but considering its mid -engineered nature and prior design trends known within 'alpha hybridization technology', (source: car comparison videos). \n",
      "- The same year, an extended Prius named after itself Prius V got close around times of almost near the original speed benchmarks from nearly all three model generations. Note this variant has a bigger footprint but also increases in vehicle length by about ten percent across every other common attributes except power spec with no clear MPC ratings at provided time mark on internet resources within my knowledge as per your query; hence some assumptions are being made due to potential measurement errors & different test results reported based mainly over an online discussion platform such Yahoo Answers alongwith forum post where Toyota representatives themselves had not clearly announced its true specifications but still remains a highly credited and accredited source, verified largely in trustworthable automobile sites like ConsumerGuide.\n",
      "- Subsequent iterations - Prius models continue with some changes across generations within MPG avergaes stay similar as to retain eco-transportation image branding wherein latest model released on record (at time from your query date back) in October 2013 has roughly accelerates only minuscule around .8~second gain & improved power with max horsepower output reaching upto full engine's potential capacity, maintain similar size class designations alongside offering extended EPA-listed electric range due primarily based upon added extra fuel cell backup storage technology providing for as additional 26 miles of zero noise drive modes before refuel needs under typical test condition which would return back into hybrid power once main genspeed demands are called on account. Thus, while raw speed remains similar across generations when directly benchmarking (if compared within controlled conditions & consistent testing environments with no specific factors altering it besides design adjustments) based off MSIP ratings for 12+ models listed out here - there has likely minor improvements in under real-world scenarios via better power and efficiency utilization but not drastically different outcome when direct assessnent was done between respective Toyota Prius variants.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "SYSTEM_MESSAGE = \"\"\"\n",
    "You are a helpful assistant that answers questions about cars based off a hybrid car data set.\n",
    "You must use the data set to answer the questions, you should not provide any information that is not in the provided sources.\n",
    "\"\"\"\n",
    "\n",
    "USER_MESSAGE = \"how fast is a prius?\"\n",
    "\n",
    "# Open the CSV and store in a list\n",
    "with open(\"hybrid.csv\", \"r\") as file:\n",
    "    reader = csv.reader(file)\n",
    "    rows = list(reader)\n",
    "\n",
    "# Normalize the user question to replace punctuation and make lowercase\n",
    "normalized_message = USER_MESSAGE.lower().replace(\"?\", \"\").replace(\"(\", \" \").replace(\")\", \" \")\n",
    "\n",
    "# Search the CSV for user question using very naive search\n",
    "words = normalized_message.split()\n",
    "matches = []\n",
    "for row in rows[1:]:\n",
    "    # if the word matches any word in row, add the row to the matches\n",
    "    if any(word in row[0].lower().split() for word in words) or any(word in row[5].lower().split() for word in words):\n",
    "        matches.append(row)\n",
    "\n",
    "# Format as a markdown table, since language models understand markdown\n",
    "matches_table = \" | \".join(rows[0]) + \"\\n\" + \" | \".join(\" --- \" for _ in range(len(rows[0]))) + \"\\n\"\n",
    "matches_table += \"\\n\".join(\" | \".join(row) for row in matches)\n",
    "\n",
    "# Now we can use the matches to generate a response\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL_NAME,\n",
    "    temperature=0.7,\n",
    "    n=1,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_MESSAGE},\n",
    "        {\"role\": \"user\", \"content\": USER_MESSAGE + \"\\nSources: \" + matches_table},\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"Response:\")\n",
    "print(f'\\x1b[32m{response.choices[0].message.content}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
